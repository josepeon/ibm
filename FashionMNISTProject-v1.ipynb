{"cells":[{"cell_type":"markdown","id":"2362746f-e6a7-4533-8835-fdc9b80292d0","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"aa1ca0ae-82ff-469c-bdee-4636c2342349","metadata":{},"outputs":[],"source":["<h1>Fashion-MNIST Project </h1>\n"]},{"cell_type":"markdown","id":"00691e8b-48a8-49fe-8773-04eff69e84ee","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n"]},{"cell_type":"markdown","id":"ec26fda6-f5ce-4ed9-81c7-8cb4a67b23d5","metadata":{},"outputs":[],"source":["<p>In this project, you will classify  Fashion-MNIST dataset using convolutional neural networks.</p>\n","<ul>\n","  \n","<ul>\n","<li><a href=\"#Preparation\">Preparation</a></li>\n","<li><a href=\"#Q1\">Questions 1: Create a Dataset Class</a></li>\n","<li><a href=\"#Q2\">Define Softmax, Criterion function, Optimizer and Train the Model</a></li>\n","\n","</ul>\n"," \n","\n","</ul>\n","\n","<p>Estimated Time Needed: <b>30 min</b></p>\n","<hr>\n"]},{"cell_type":"markdown","id":"74a0ad03-954e-4845-bbde-fde818c43388","metadata":{},"outputs":[],"source":["<a name=\"Preparation\"><h2 id=\"Preparation\" >Preparation</h2></a>\n"]},{"cell_type":"markdown","id":"4ad252dc-5cef-46d9-9c48-12ce0c090cc9","metadata":{},"outputs":[],"source":["Download the datasets you needed for this lab.\n"]},{"cell_type":"markdown","id":"5b9f1b73-12f2-441f-80cb-a1a64732a6b7","metadata":{},"outputs":[],"source":["The following are the PyTorch modules you are going to need\n"]},{"cell_type":"code","id":"61d6f63b-30bf-400b-8ec4-a26b1e1a27fa","metadata":{},"outputs":[],"source":["%%time\n%pip install pandas numpy matplotlib\n%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n    --index-url https://download.pytorch.org/whl/cpu"]},{"cell_type":"code","id":"caaa3695-2ccb-46c1-8e64-424b147fd842","metadata":{},"outputs":[],"source":["# !pip install torch\n# !pip install torchvision\n# !pip install matplotlib"]},{"cell_type":"code","id":"adfb66e4-1c61-49f6-a504-733cec130184","metadata":{},"outputs":[],"source":["# PyTorch Modules you need for this lab\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"785159f3-f054-4c3a-805d-17a22f6c3784","metadata":{},"outputs":[],"source":["Import Non-PyTorch Modules \n"]},{"cell_type":"code","id":"79861fb1-bc7e-461d-9862-aae70659e5ad","metadata":{},"outputs":[],"source":["# Other non-PyTorch Modules\n\nfrom matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\n\nfrom PIL import Image"]},{"cell_type":"code","id":"cd2bc839-b789-4a2d-9543-d002492a180f","metadata":{},"outputs":[],"source":["def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))"]},{"cell_type":"markdown","id":"dfb8da66-32f2-4801-a756-d237bbe8a73c","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"0c022158-061d-4c8c-80c7-2b89587f81e1","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"4fe4b23f-74e3-4c87-b850-df8392160dfb","metadata":{},"outputs":[],"source":["<a name=\"Q1\"><h2 id=\"Q1\">Questions 1: Create a Dataset Class</h2></a>\n"]},{"cell_type":"markdown","id":"916a6a76-6b57-442b-87a3-2a6d1480aa7e","metadata":{},"outputs":[],"source":["In this section, you will load a Dataset object, but first you must transform the dataset. Use the <code>Compose</code> function to perform the following transforms:. \n","<ol>\n","    <li>Use the transforms object to<code> Resize </code> to resize the image.</li>\n","    <li>Use the transforms object to<code> ToTensor </code> to convert the image to a tensor.</li>\n","</ol>\n","\n","You will then take a screen shot of your validation data.\n"]},{"cell_type":"markdown","id":"e4d2d9ff-9e06-438d-8dc8-5d3acd3e3399","metadata":{},"outputs":[],"source":["Use the Compose function to compose the transforms\n"]},{"cell_type":"code","id":"b28356d8-a13a-436f-9e2a-948e8b9665b6","metadata":{},"outputs":[],"source":["#Hint:\n\nIMAGE_SIZE = 16\n\ntransforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\ntransforms.ToTensor()#\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"]},{"cell_type":"markdown","id":"95989521-fe35-4837-929e-3a6158b51a9a","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"80697777-52b1-4055-b181-40783f073f1e","metadata":{},"outputs":[],"source":["Create two dataset objects for the Fashion MNIST  dataset. One for training data called <code> dataset_train </code> and one for validation data <code>dataset_val</code>. You will be asked to take a screenshot of several samples.\n"]},{"cell_type":"markdown","id":"f173c1b5-1748-4bc5-aebb-40fcce0867f0","metadata":{},"outputs":[],"source":["<b>Hint:</b>\n","<code>dsets.FashionMNIST(root= '.fashion/data', train=???, transform=composed,  download=True)</code>\n"]},{"cell_type":"code","id":"7a81c758-0512-460b-af06-17833f51741f","metadata":{},"outputs":[],"source":["# Enter your code here\n"]},{"cell_type":"code","id":"c90877d2-0765-4fe9-a5e6-c0d2c1855057","metadata":{},"outputs":[],"source":["for n,data_sample in enumerate(dataset_val):\n\n    show_data(data_sample)\n    plt.show()\n    if n==2:\n        break "]},{"cell_type":"markdown","id":"e0d86c3e-fc79-4573-814a-e3a2f130b9f0","metadata":{},"outputs":[],"source":["<a name=\"Q2\"><h2 id=\"Q2\">Questions 2</h2></a>\n","Create a Convolutional Neural Network class using ONE of the following constructors.  Train the network using the provided code then provide a screenshot of your training cost and accuracy with your validation data.\n"]},{"cell_type":"markdown","id":"41eea87b-6a94-48ce-aa46-cfeda616e5f7","metadata":{},"outputs":[],"source":["Constructor  using Batch Norm \n"]},{"cell_type":"code","id":"7b032680-e160-4739-b5c5-835b73d8c76f","metadata":{},"outputs":[],"source":["class CNN_batch(nn.Module):\n    \n    # Constructor\n    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n        super(CNN_batch, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.conv1_bn = nn.BatchNorm2d(out_1)\n\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n        \n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.conv2_bn = nn.BatchNorm2d(out_2)\n\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n        self.bn_fc1 = nn.BatchNorm1d(10)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x=self.conv1_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x=self.conv2_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x=self.bn_fc1(x)\n        return x"]},{"cell_type":"markdown","id":"c6fcb258-983b-4cb2-9bd7-3fee7e170b5b","metadata":{},"outputs":[],"source":["Constructor  for regular Convolutional Neural Network\n"]},{"cell_type":"code","id":"a23e96a2-70ca-48bb-b9c6-1171bd436680","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    \n    # Constructor\n    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x"]},{"cell_type":"markdown","id":"7ff6a521-a705-4473-bc9d-0a1c349cff8f","metadata":{},"outputs":[],"source":["train loader  and validation loader \n"]},{"cell_type":"code","id":"7fb61b9c-844d-4dcc-935a-f65f273b77ea","metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=100 )\ntest_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=100 )"]},{"cell_type":"markdown","id":"7fd38757-ff72-47a8-b53c-761196f07430","metadata":{},"outputs":[],"source":["Convolutional Neural Network object \n"]},{"cell_type":"code","id":"59f34764-c741-45c2-94f4-8c93d38d2e55","metadata":{},"outputs":[],"source":["#model = CNN(out_1=16, out_2=32,number_of_classes=10)\n#model =CNN_batch(out_1=16, out_2=32,number_of_classes=10)"]},{"cell_type":"markdown","id":"3b056110-1570-4963-85b5-765d9597b335","metadata":{},"outputs":[],"source":["Create the objects for the criterion and the optimizer named <code>criterion</code> and <code>optimizer</code>. Make the optimizer use SGD with a learning rate of 0.1 and the optimizer use Cross Entropy Loss\n"]},{"cell_type":"code","id":"c15fb0e2-fdea-4d71-ae67-2727cf152a0c","metadata":{},"outputs":[],"source":["# Enter your code here"]},{"cell_type":"markdown","id":"e7f92188-b297-49d2-8b33-550435002d68","metadata":{},"outputs":[],"source":["Code used to train the model \n"]},{"cell_type":"code","id":"83cabeee-84ed-4794-aa38-27a9f0651445","metadata":{},"outputs":[],"source":["import time\nstart_time = time.time()\n\ncost_list=[]\naccuracy_list=[]\nN_test=len(dataset_val)\nn_epochs=5\nfor epoch in range(n_epochs):\n    cost=0\n    model.train()\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        z = model(x)\n        loss = criterion(z, y)\n        loss.backward()\n        optimizer.step()\n        cost+=loss.item()\n    correct=0\n    #perform a prediction on the validation  data \n    model.eval()\n    for x_test, y_test in test_loader:\n        z = model(x_test)\n        _, yhat = torch.max(z.data, 1)\n        correct += (yhat == y_test).sum().item()\n    accuracy = correct / N_test\n    accuracy_list.append(accuracy)\n    cost_list.append(cost)\n    "]},{"cell_type":"markdown","id":"b21d2004-bf06-422c-93df-a10b53b270c9","metadata":{},"outputs":[],"source":["You will use the following to plot the Cost and accuracy for each epoch for the training and testing data, respectively. \n"]},{"cell_type":"code","id":"b194c50c-2cf4-419d-9b6b-462d33c66685","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list, color=color)\nax1.set_xlabel('epoch', color=color)\nax1.set_ylabel('Cost', color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color) \nax2.set_xlabel('epoch', color=color)\nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"a6fb29f3-a43d-4afa-91bc-678ec987ff41","metadata":{},"outputs":[],"source":["dataset: https://github.com/zalandoresearch/fashion-mnist\n"]},{"cell_type":"markdown","id":"56e81c06-bfce-42c7-9aa9-8d132ecd9de4","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"4c3b482b-9ab0-4b24-a375-445d84b6ae93","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a> \n"]},{"cell_type":"markdown","id":"84b8b1be-771c-4914-a01f-d7ebcd4db85d","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"3bb388bb-d40d-4c61-9306-2e128f73ae43","metadata":{},"outputs":[],"source":["## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"f6dcb27f199dff848612e9e61ce1dbfeb0be13836cd1057108e853fd5463fa28"},"nbformat":4,"nbformat_minor":4}